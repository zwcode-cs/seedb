%!TEX root=document.tex

% \section{View Pruning}
% \label{sec:pruning}

\techreportfinal{
\subsection{Offline Pruning.}
Even before any queries are issued to \SeeDB, we
have the ability to identify clusters of attributes
that are strongly correlated with each other, 
such that if we elect to display a visualization 
on one of them, we can avoid 
displaying visualizations on others
(since they would be redundant).
Consider for example a flight dataset that 
stores names of airports as well as the corresponding
airport codes.
Since names of airports and airport codes have
a 1:1 relationship, generating, say, 
average delay by airport name, and
average delay by airport code would lead
to identical visualizations. 
Consequently, it would suffice to compute and recommend
only one of these visualizations.
% To prune redundant views, we
% cluster attributes by degree of correlation,
% perform clique detection and choose a representative
% view from every clique.

We adopt the following steps to prune redundant views: 
(1) For each table, we first determine the entire space of aggregate views. 
(2) Next, we prune all aggregate views containing attributes with 0 or low 
variance since corresponding visualizations are unlikely to be interesting.
(3) For each remaining view $V_i$, we compute the distribution 
$P[V_i (D)]$ for reference views on the entire dataset $D$.
(4) The resulting distributions are then clustered based on pairwise
correlation.
(5) From each cluster, we pick one view to compute as a cluster 
representative and store ``stubs'' of clustered views for subsequent use.
At run time, the view generator accesses previously generated view stubs, 
removes redundant views and passes the remaining stubs to the execution engine.
%\agp{Also, would be good to have some experimental figures or more details on this}
% since the rest would contain redundant information.
% \papertext{We describe how we perform
% offline pruning, and describe the experimental
% gains from offline pruning in our extended technical report.
% At a high-level, we use clustering of attributes,
% followed by clique detection to detect
% and eliminate redundant aggregate views.}
Offline pruning allows us to significantly 
reduce the number of views (and execution time) in real
datasets; for the DIAB dataset described in Section~\ref{sec:experiments},
we reduce the number of possible views from 72 to 41 (45\% reduction), and from
70 to 54 (25\% reduction) for the BANK dataset. 
}

% Intuitively, this pruning is important because fewer views equates to better
% performance.
% Note that the pruning performed by the view generator is based on redundancy while that performed by the execution engine is based on low utility.
% We distinguish between the pruning performed by the execution engine and the pruning
% performed by the view generator: the execution engine prunes {\it low-utility} views
% while the view generator prunes {\it redundant} views. 
% A set of views $\{V\}$ is said to be redundant if all views in $\{V\}$ have
% similar distributions and are therefore expected to have similar utility.
% For instance, consider a hypothetical dataset that stores the names of airports along
% with the corresponding airport code. 
% Views produced by grouping along either of these attributes will be identical.
% Similarly, Figure \ref{} shows three views of a real-world dataset (the DIAB dataset 
% from Section \ref{sec:experiments}).
% We observe that these views are very similar and hence it would suffice to compute
% and recommend only a representative view (while noting similar views).

% An extreme example of redundant views is the following: a table stores the sales
% of a product as measured in US Dollars and measured in Euros. 
% Views with either of these column as the measure attribute will be identical and
% have the same utility.
% Similarly, if a table has two dimension attributes, one corresponding to the
% airport name and another corresponding to the airport code, views with either
% of these attributes as dimensions are guaranteed to produce identical views.
% In both of the above cases, it suffices to compute and show a single view
% that is representative of multiple views (and list redundant
% views that have been omitted).


% We next compute pairwise correlations between these vectors (note: correlation is
% only defined for equi-sized vectors) and cluster views based on this correlation.
% Views in each cluster are expected to produce views with very similar distributions
% and therefore utilities.
% As a result, we pick only a single view from every cluster.
% Stubs corresponding to the selected views are stored compactly on disk for subsequent use.


% Next it computes the correlation between distributions corresponding to each
% pair of views (note that the dimensions must have same cardinality for this
% comparison to be valid).
% Correlation scores are thresholded and used to cluster views into groups.
% Finally, we pick a single view from every view cluster.


% Details of our pruning algorithm are presented in Appendix
% \ref{sec:view_pruning} along with examples of view clusters.


% In this section, we discuss an important component of \SeeDB\ that is invoked
% even before the Execution Engine runs, namely the View Generator.
% Given a user query $Q$, the purpose of the view generator module is to take the
% input query, obtain metadata about the underlying tables and use correlations
% between different columns in the table to prune views whose evaluation is
% unnecessary. 
% We distinguish between the pruning done in the execution engine and the pruning
% done in the view generator: the execution engine prunes away low-utility views
% while the view generator prunes away {\it redundant} views.
% So what are redundant views? Redundant views are views that have similar
% distributions and are therefore expected to have similar utility.
% An extreme example is that of sales of a product as measured in US
% Dollars and measured in Euros. Views with these measure attributes will be
% identical and have the same utility.
% Similarly, two dimension attributes, one corresponding to the airport name
% and another corresponding to the airport code are guaranteed to produce identical
% views irrespective of the measure attribute.
% In both of the above cases, it suffices to compute and show only a single view
% that is representative of multiple views (the frontend does list redundant
% views that have been omitted).

% The View Generator works in two stages: it performs view pruning offline and
% identifies the set of viable views; then, when a new user query comes in, it
% reads the set of viable views, performs pruning based on the input query and
% passes view stubs on to the Execution Engine. 
% The offline pruning does not depend on the input query and can therefore be
% perfomed only once.
% The offline stage works as follows. 

% First, for each table, the View Generator obtains various types of
% metadata including the data types of attributes, their classification into
% measure and dimension attributes, number of distinct values for dimension
% attributes, and the distributions for each measure attribute (mean, std
% deviation).
% The first piece of metadata is essential to determine the full space of
% views. 
% The number of distinct values for dimension attributes and the variance for
% measure attributes is used to perform basic pruning of views (e.g. views
% containing zero variance attributes will have low utility).

% \mpv{If two dimension attributes $a_i$ and $a_j$ have
% a high degree of correlation (e.g. full name of airport and abbreviated name of
% airport), the views generated by grouping the table on $a_i$ and $a_j$ will be
% very similar (and have almost equal utility). We can therefore generate and
% evaluate a single view representing both $a_i$ and $a_j$. \SeeDB\ clusters
% attributes based on correlation and evaluates a representative view per
% cluster.}



  
% We next describe a scheme that allows us to associate upper and lower bounds for
% views by evaluating them on a small sample of the dataset.
% We describe the use of the scheme on a simple view where AVG(Y) for a given
% attribute Y is being computed for each group in attribute X.
% We can then depict this view using a bar chart or a histogram.
% 
% For this derivation, we assume that the AVG(Y) for any X = $x_i$, is normally
% distributed around a certain mean $p$.
% Given a number of samples for Y for X = $x_i$, we can employ the following
% theorem \cite{stats_book} to bound $p$ within a confidence interval with
% probability $1 - \delta$:
% \begin{theorem}~\label{thm:confint}
% If $\hat{p}$ and $s$ are the mean and standard deviation 
% of a random sample of size $n$ from a normal distribution with unknown 
% variance, a $1 - \delta$ probability confidence interval
% on $p$ is given by:
% $$\hat{p} - \frac{t_{\delta/2, n-1} s}{\sqrt{n}} \leq p \leq \hat{p} + \frac{t_{\delta/2, n-1} s}{\sqrt{n}}$$
% where $t_{\delta/2, n-1}$ is the upper 100$\alpha/2$ percentage point
% of the $t$-distribution with $n-1$ degrees of freedom.
% \end{theorem}
% 
% Now, we demonstrate how we can use this theorem to establish an upper 
% and lower bound for the utility of a view, with probability $1 - \delta$.
% 
% Let the distance vector corresponding to the target view be:
% $\bar{a} = [a_1, a_2, \ldots, a_k]$ while the distance vector corresponding to
% the comparison view is:
% $\bar{b} = [b_1, b_2, \ldots, b_k]$.
% Notice that on very large datasets, it may be beneficial to precompute the
% distance vectors corresponding to the comparison views, so we assume that the
% vector $\bar{b}$ is computed exactly and known in advance.
% We let $a = \sum_i a_i$, and $ b = \sum_i b_i$.
% 
% Our goal is to use the sample to bound the values of the $a_i$ around $\ha_i$
% such that we can establish upper and lower bounds for the utilities.
% By applying Theorem~\ref{thm:confint}, we
% can get values $c_i$ for which $a_i \in [\ha_i - c_i, \ha_i + c_i]$
% with probability greater than $1 - \delta/k$.
% (By union bound, we will be able to ensure that all $a_i$'s
% are in their intervals with probability $1 - \delta$.)
% 
% Now, given these values $c_i$, we can establish an upper bound for the
% EMD (and also similarly for other distance metrics) in the following manner:
% We let $q_1(\bar{a}) = \sum_i \ha_i - c_i$, and $q_2(\bar{a}) = \sum_i \ha_i + c_i$.
% 
% 
% \begin{align*}
% EMD(\bar{a}, \bar{b}) & = \sum_i |a_i / a - b_i / b|\\
%           & = \sum_i |a_i / a - b_i / b|\\
%           & = 1/ab \sum_i \max (a_ib  - b_ia, b_ia - a_ib)\\
% \end{align*}
% Thus, we have:
% \begin{align}
% \frac{1}{b q_1(\bar{a})} \sum_i \max (a_ib  - b_ia, b_ia - a_ib) \leq & EMD(\bar{a}, \bar{b}) \leq \frac{1}{b q_2(\bar{a})} \sum_i \max (a_ib  - b_ia, b_ia - a_ib)\label{eq:emd}
% \end{align}
% 
% Note that: 
% \begin{align*}
% (\ha_i - c_i)b  - b_i (\sum_i (\ha_i + c_i)) & \leq a_ib  - b_ia  \leq (\ha_i + c_i)b  - b_i (\sum_i (\ha_i - c_i)), \textrm{\ and} \\
% b_i (\sum_i (\ha_i - c_i)) - (\ha_i + c_i) b & \leq b_i a  - a_i b  \leq  b_i (\sum_i (\ha_i + c_i)) - (\ha_i - c_i) b
% \end{align*}
% By plugging these quantities back into Eq~\ref{eq:emd},
% we have upper and lower bounds on the EMD metric.
% Similar mechanisms may be used to derive upper and lower bounds for other metrics.
% 
% Now that we have upper and lower bounds for the utility of each target view
% by evaluating the query on a sample,
% we can easily use it to prune away a number of views that are definitely not likely to be part 
% of the top-K,
% and instead focus on views that may be part of the top-K.

%   \subsection {Partitioning Tables}
%   The increase in the total execution time when a large number of queries are
%   executed in parallel suggests that there is a ``sweet spot'' with respect to
%   the maximum number of queries that can be run in parallel on a given table.
%   Therefore, we uniformly partition large tables into smaller ones and run
%   subsets of queries against each of the partitions. Note that the views
%   returned are nor approximate because we are now executing views against
%   subsets of the data. As a result, bounds developed in sampling now apply. We

 


%If a dimension attribute $\mathcal{d}$ is highly correlated with measure
  %attribute $\mathcal{m}$, then?

% \mpv{also from full paper draft}
% It is possible to collect the above statistics at the dataset level too, as
% opposed to the entire table level. The advantage of table level statistics is
% that they have to be computed only once per table; however, dataset-level
% statistics are more accurate since they only consider the specific parts of the
% table. XXX: we use dataset-level statistics with table statistics do not result
% in aggressive pruning. 



