%!TEX root=document.tex


\section{Related Work}
\label{sec:related_work}
\SeeDB\ draws on related work from multiple areas;
we review papers in each of the areas, and describe how they relate to
\SeeDB. 

\stitle{Visualization Tools}:
The visualization research community has introduced a number of
visualization toolkits and tools, including Spotfire and 
Tableau~\cite{polaris, Ahlberg:1996:SIE:245882.245893}, that enable
users to build visualizations with different levels of sophistication.
Similar visual specification tools have been introduced by the
database community as well, e.g., Fusion
Tables~\cite{DBLP:conf/sigmod/GonzalezHJLMSSG10}.
Some of these tools provide features that use heuristics to suggest
chart types for a given set of variables. 
% but these features are restricted to a set of aesthetic rules of thumb that
% guide which visualization is most appropriate.
% In these tools, the goal is to make it simpler for analysts 
% to pose specifications of what they would like to examine
% (e.g., they would like to examine sales by year for cars),
% and the visual analytics tool would provide an appropriate
% visualization medium for this visualization specification
% (e.g., a trend line or a bar chart).
% Most of the ``smarts'' in these tools is restricted
%to a set of 
%given a visual specification.
All of these tools, however, require the user to manually specify
visualizations, leading to a tedious trial-and-error process to find
visualizations that are interesting or useful.
% In all these tools, the user must choose the data they want to visualize, requiring
% a tedious iteration through all sets of attributes.
In contrast, \SeeDB seeks to automatically identify and 
recommend interesting visualizations based on a utility metric.

% based on a generalized distance function, finding
% attribute sets that maximize the value of this function.
%in addition to the simple visualization
%specification functionalities described above,
%which is more appropriate when the analyst already
%knows which visualizations they should be examining.

% SRM -- I don't think anyone is going to say that Matlab does what we do
%Statistical analysis and graphing packages such as R, SAS and Matlab could also
%be used generate visualizations, but they lack the ability to filter and
%recommend visualizations. 


\mpv{Still working
\stitle{Recommending Visualizations.}
%A few systems have attempted to automate some aspects of data analysis
%and visualization. For example, 
Profiler 
detects anomalies in data \cite{DBLP:conf/avi/KandelPPHH12} and provides
some visualization recommendation functionality,
but is restricted determining the best binning for the
for the $x$ axis: in particular, it decides which granularity
is appropriate to bin on to depict the most interesting relationships 
between data. 
Since this is a much simpler problem
than ours, sophisticated techniques are not necessary.
% Our work is also similar to VizDeck which is a tool that given a dataset, uses a
% set of pre-determined rules to create diverse visualizations and
% allows the user to pick and choose the visualizations that seem relevant
% \cite{DBLP:conf/sigmod/KeyHPA12}.
% Thus, while powerful, VizDeck requires much more manual input than \SeeDB. 
% In addition, the visualizations generated by VizDeck do not leverage the
% context of the underlying dataset, making the visualizations generated by
% both systems very different in flavor. 
% It would be instructive to augment
% VizDeck visualizations with \SeeDB\ visualizations to study their relative
% utility.
%Another related tool 
%is 
VizDeck~\cite{DBLP:conf/sigmod/KeyHPA12}
depicts all possible 2-D visualizations of a dataset on a dashboard.
While a dashboard with all 2-D visualizations can provide an overview of the data, 
this approach can quickly become intractable as the number of attributes increases.}
% The rendering of all visualizations also limits it to datasets with a small 
% number of attributes.
% Given that VizDeck generates all visualizations, it is  meant for 
% small datasets; additionally, \cite{DBLP:conf/sigmod/KeyHPA12} does not discuss techniques
% to speed-up the generation of these visualizations. 


\stitle{Scalable Visualizations.}  There has been some recent work on
scalable visualizations that employ in-memory caching, sampling, and
pre-fetching to improve the interactivity of visualization systems
backed by
databases (e.g.,~\cite{doshi2003prefetching,DBLP:journals/corr/KimBPIMR14}).
Such techniques could be employed in our settings to further improve
response times (although some of these techniques, such as in-memory caching,
can only work with small datasets).


% . Immens~\cite{2013-immens} and Profiler (mentioned above)
% maintain a data cube in memory and use it to support rapid user
% interactions. While this approach is possible when the dimensionality
% and cardinality is small (e.g., simple map visualizations of a single
% attribute) it cannot be used with large tables and ad-hoc queries.
% Pre-computation and pre-fetching are two other techniques that have
% been used for scalability, e.g., \cite{hotmap} uses precomputed image
% tiles for geographic visualization, \cite{doshi2003prefetching} uses
% extensive pre-fetching and caching.  Our recent paper describes
% techniques for generating visualizations approximately, but with
% ordering guarantees~\cite{DBLP:journals/corr/KimBPIMR14}; these
% techniques could be easily employed in our setting to further improve
% the response times.  
%A recent paper \cite{2014-viz-latency} discusses
%how high latency in visualization systems reduces the rate at which
%users observe, analyze and draw conclusions from data, thus making a
%strong case for interactive response times.

%Finally, finding interesting visualizations in data also involves understanding
%user preferences. 
%In future work, we plan to learn user preference models towards visualizations
%using techniques similar to \cite{CHI:YangLZ14, IUIGotzW09}. 


% \agpneutral{Other recent work has addressed other aspects of visualization
% scalability, including prefetching and caching~\cite{doshi2003prefetching}, data
% reduction~\cite{burtini2013time} leveraging time series data
% mining~\cite{esling2012time}, clustering and
% sorting~\cite{guo2003coordinating,seo2005rank}, and dimension
% reduction~\cite{Yang:2003:VHD:769922.769924}. These techniques are orthogonal to
% our work, which focuses on speeding up the computation of a single visualization
% online.}\\

\stitle{Data Cube Materialization:} 
%OLAP (Online Analytical Processing)~\cite{olap} 
%concerns itself with the 
Computations on {\em data cubes}~\cite{DBLP:jounral/DMKD/GrayCBLR97}
involve aggregation across multiple dimensions.
%,
%considering all possible groupings of attributes,
%at different granularities, and all possible aggregations of other attributes.
%These data cubes are used for report generation for Business Intelligence
%(BI) applications.
Even when the number of attributes and number
of distinct values of each attribute is relatively small,
the space required to materialize then entire cube can be prohibitive,
meaning that only a few (if any) dimensions can be pre-aggregated.
%and store these data cubes is
%very large --- as a result of this, in practice, data cubes
%are only stored, if at all, for a small number of attributes 
%(typically less than a handful).
There has been some work on identifying, given a query workload,
which cubes to materialize within a certain storage budget,
so as to minimize the amount of work to be performed at run time
~\cite{DBLP:conf/VLDB/AgarwalADG96,DBLP:conf/SIGMOD/HarinarayanRU96}.
%In our setting, since the number of attributes can be rather large,
%and since any ad-hoc data exploration queries can be provided on-the-fly, materialization
%is not an option except for the most common or popular attributes.
While the optimization techniques underlying cube materialization 
are similar in spirit to our sharing optimizations from Section~\ref{sec:sharing_opt},
they focus on offline computation of views to minimize storage rather than efficient online
optimization.

% (but not similar to our pruning optimizations).
% However, the underlying techniques are different from ours in two ways:
% (1) Cube materialization computes
%  these aggregates offline, as opposed to online: the bottleneck
% is not computation, but storage.
% (2) We must compute the results for
% all possible views, cube materialization only seeks 
% to compute a few (good) sub-cubes. Our algorithms focus on
% grouping views into batches,
% each of which fit within our resource constraints. 
% In cube materialization there is a single ``batch'' of aggregates
% that is computed offline (rather than multiple batches) and then stored.


\stitle{Browsing Data Cubes:}
There has been some  work on using data mining techniques
to aid in the exploration of data 
cubes~\cite{DBLP:conf/vldb/Sarawagi99, DBLP:conf/vldb/SatheS01, DBLP:conf/vldb/Sarawagi00, 
DBLP:conf/SIGKDD/OrdonezC09}.
Sarawagi et al.~\cite{DBLP:conf/EDBT/SarawagiAM98, DBLP:conf/vldb/Sarawagi00} 
 explored 
the question of finding ``interesting'' cells in a cube.
The interestingness of a cell is defined by how surprising
its value is given the other values in the cube:
\cite{DBLP:conf/EDBT/SarawagiAM98} uses techniques 
based on a table analysis method while
\cite{DBLP:conf/vldb/Sarawagi00} uses techniques based on entropy to find interesting cells.
These techniques generally identify sub-cubes of a cube that have the most
deviation amongst all sub-cubes, analogous to SeeDB finding dimension attributes that show
the greatest variation in a given
aggregation query.  
Instead of identifying interesting sub-cubes within a cube, SeeDB focuses on
finding variation vis-a-vis a reference data set, recommending multiple views over a 
large set of possible visualizations.

%Inherent interestingness is a much simpler problem than the
%one addressed in our paper, since it can be precomputed before
%any queries arrive. (This is because inherent interestingness is independent of queries.) 
%Due to this, 
%the techniques in browsing \cite{DBLP:conf/EDBT/SarawagiAM98, 
%DBLP:conf/vldb/Sarawagi00} cannot be applied in our context.
%Furthermore, instead of comparing individual cells, 
%\SeeDB\ evaluates sets of aggregates (i.e. distributions
%for our view), thus focusing on trends in values rather than individual values:
%unlike individual cube values, trends can be more easily displayed
%and examined on visual interfaces.


In \cite{DBLP:conf/vldb/Sarawagi99}, Sarawagi
proposes techniques to explain an
increase or decrease in a specific aggregate by drilling down into that aggregate.
In contrast, \SeeDB\ seeks to find interesting
differences between two datasets that have not yet been aggregated along any dimensions.
%,
% rather than explaining the cause of variations observed in already aggregated
%quantities.  
Wu et al~\cite{scorpion} tackle a similar problem in Scorpion,
and differ for similar reasons.
%\agp{The prev paragraph is quite weak. I'll need to look into the paper to identify the 
%differences.}

% Second, instead of finding ``intrinsically'' interesting aggregates, 
% \SeeDB\ evaluates aggregate views with respect to a comparison dataset. 

% \agp{If we do have a general metric, get rid of sentence below.}
% We note however that incorporating an ``intrinsic'' interesting-ness metric into our utility function is an avenue for future work.


\stitle{Multi-Query Optimization:} Our batching optimizations
%presented in Section~\ref{sec:sharing_opt} 
draw on
related techniques from literature on shared
scans~\cite{Fernandez:1994:RBW:191843.191947} and multi-query
optimization (e.g.
~\cite{DBLP:journals/tods/Sellis88}).
%{\em shared scans}~\cite{DBLP:conf/VLDB/ZukowskiHNB07,DBLP:conf/sigmod/HollowayRSD07,DBLP:journals/pvldb/UnterbrunnerGAFK09,DBLP:conf/icde/RamanSQRDKNS08,DBLP:journals/pvldb/QiaoRRHL08}
% Our goal of evaluating
%many views in parallel is an instance of multi-query 
%optimization,
%i.e., optimizing the execution of a collection of queries rather
%than one at a time.
%Multi-query optimization focuses on the creation of ``global''
%query plans for a collection of queries rather than a single one,
%such that the intermediate operators and results are shared across queries.
Our problem is simpler however, since we don't have 
to wait for a batch of queries to arrive and all queries
are aggregations. 
The goal of choosing only the top visualizations also enables us
to use pruning techniques to terminate evaluation of low utility visualizations, 
something other multi-query schemes cannot do.

\stitle{Query Recommendation Systems}: There is related work on recommending queries in databases 
(see~\cite{marcel2011survey}).  Such systems are
 designed to help users pose users relevant
queries over of a database, typically by consulting historical query
workloads and using statistical similarity or recommender algorithms
to refine user inputs.  
While these techniques focus on recommending SQL queries instead of visualizations
(and hence don't focus on visually relevant utility metrics),  we
believe they could be integrated into a generalized utility metric
inside SeeDB.
% , although a full user-study comparing their effectiveness
% is outside the scope of this work.





% The work done in \SeeDB\ is similar to previous literature in
% browsing OLAP data cubes. 
% Instead of building complete data cubes,
% one can think of \SeeDB\ views as projections of the cube along various
% dimensions.
%  Data cubes have been very well studied in the literature
% \cite{DBLP:conf/SIGMOD/HarinarayanRU96, DBLP:jounral/DMKD/GrayCBLR97}, and work such as
% ~\cite{DBLP:conf/vldb/Sarawagi99, DBLP:conf/vldb/SatheS01,
% DBLP:conf/vldb/Sarawagi00, DBLP:conf/SIGKDD/OrdonezC09} has explored the
% questions of allowing analysts to find explanations for trends, get suggest for
% cubes to visit, identify generalizations or patterns starting from a single
% cube. 
% This literature is not directly applicable to our problem since the cubes we
% are considering have 10s to 100s of dimensions, making traditional cube
% algorithms infeasible. 


% \stitle{General Purpose Data Analysis Tools:}
% Our work is also related to data mining and the work on building general purpose
% data analysis tools on top of databases. 
% For example, MADLib \cite{DBLP:conf/VLDB/HellersteinRSWF12}
% implements various analytic functions inside the database. 
% MLBase similarly
% \cite{DBLP:conf/CIDR/KraskaTDGFJ2013} provides a platform that allows users to
% run various machine learning algorithms on top of the Spark system
% \cite{DBLP:conf/SCC/ZahariaCFSS10}.


% \techreport{
% \stitle{Other Related Work:}
% The techniques we use in our custom implementation of \SeeDB\ is
% related to work in top-k ranking and multi-armed bandits.
% The confidence interval-based technique discussed in Section \ref{sec:confidence_interval} 
% is similar to top-k based pruning algorithms developed 
% in other contexts~\cite{DBLP:conf/pods/FaginLN01, 
% DBLP:conf/vldb/IlyasAE04, DBLP:conf/ICDE/ReDS07}.
% %and similar to our work on  
% %sampling for visualizations~\cite{DBLP:journals/corr/KimBPIMR14}.
% The techniques in these papers on top-$k$ pruning are 
% tailored more towards picking individual desirable tuples,
% rather than visualizations (which correspond to an array of aggregates),
% and thus the techniques are very different.
% %The emphasis in the latter work is to ensure the generation of
% %approximate visualizations with certain guarantees---here,
% %our goal is to evaluate visualizations (possibly approximately). 
% Similarly, multi-armed bandits (see Section \ref{sec:multi_armed_bandit}) is another
% related area of research. 
% Our problem presents a novel application of multi-armed bandit strategies to 
% exploratory data analysis.
% The technique we adopt is from a recent paper on top-$k$ MAB~\cite{BubeckWV13,audibert2010best} and is a 
% a variant of the original UCB algorithm \cite{AuerCF02, LaiR85}.}
