%!TEX root=document.tex

\section{View Pruning}
\label{subsec:pruning}

Given a user query $Q$, the goal of the view generator module is to use metadata
to determine the space of all possible views and then use correlations between
data distributions to prune views whose evaluation is unnecessary. \SeeDB\
utilizes the following metadata information: (1) data types of attributes, and
their classification into measure and dimension attributes, (2) number of
distinct values for dimension attributes, (3) the
distributions for each measure attribute (mean, std deviation), and (4) the
correlation between attributes.

The first metadata piece is essential for \SeeDB\ to determine the full space of
views to consider. 
Remember from Section \ref{}, that \SeeDB\ considers all
views of the form $(a, m, f)$ where $a$ is a dimension attribute, $m$ is a
measure attribute, and $f$ is the aggregate function. 
The number of distinct values for dimension attributes and the variance for
measure attributes is used to perform basic pruning of views (e.g. views
containing zero variance attributes will have low utility).
<<INSERT PICTURE>>

The most significant pruning power we obtain is by examining the correlation
between views of different attributes.
The idea is to identify views whose distributions are highly correlated so that
instead of evaluating each one of those views independently, we can evaluate
only a representative view. 
We follow the following pruning technique: for all possible $(a, m, f)$ triples,
we compute the distribution for each view over the entire data (i.e.
the comparison view). 
Once the distributions for all views have been computed, we compute the
correlation between distributions of views containing the same dimension
attribute. 
Specifically, since the distributions are numeric, we compute the pairwise
Pearson correlation between all views with the same dimension attribute.
Next, we (conceptually) build a graph of all views containing the
same dimension attribute. 
The nodes of this graph are views and an edge exists between two nodes if the
correlation between the two views is greater than a threshold (set to 0.95 in
our experiments). 
We identify cliques (and almost cliques) in this graph and select a single node
(i.e. view) from each clique as a representative view for that clique. 
The
remaining views are pruned as they will not provide extra information to the
user. 
Note that although these views are pruned during processing, the \SeeDB\
frontend communicates this information to the user by providing a list of similar views.
In real datasets, we find that this pruning significantly reduces the
number of views that must be considered.
For example, in the diabetes dataset discussed in Section \ref{}, the total
number of views is reduced from XXX to XXX due to this pruning strategy, while
for the banking dataset, the total views are reduced from XXX to XXX.

The view generator uses the metadata and pruning strategies discussed above to
create view stubs for each view that has not been pruned. 
These are the views that will be evaluated by the execution engine in the next
step.
In the next section, we discuss in detail both the execution engines supported
by \SeeDB\ and engine specific optimizations used.

\mpv{If two dimension attributes $a_i$ and $a_j$ have
a high degree of correlation (e.g. full name of airport and abbreviated name of
airport), the views generated by grouping the table on $a_i$ and $a_j$ will be
very similar (and have almost equal utility). We can therefore generate and
evaluate a single view representing both $a_i$ and $a_j$. \SeeDB\ clusters
attributes based on correlation and evaluates a representative view per
cluster.}



  
% We next describe a scheme that allows us to associate upper and lower bounds for
% views by evaluating them on a small sample of the dataset.
% We describe the use of the scheme on a simple view where AVG(Y) for a given
% attribute Y is being computed for each group in attribute X.
% We can then depict this view using a bar chart or a histogram.
% 
% For this derivation, we assume that the AVG(Y) for any X = $x_i$, is normally
% distributed around a certain mean $p$.
% Given a number of samples for Y for X = $x_i$, we can employ the following
% theorem \cite{stats_book} to bound $p$ within a confidence interval with
% probability $1 - \delta$:
% \begin{theorem}~\label{thm:confint}
% If $\hat{p}$ and $s$ are the mean and standard deviation 
% of a random sample of size $n$ from a normal distribution with unknown 
% variance, a $1 - \delta$ probability confidence interval
% on $p$ is given by:
% $$\hat{p} - \frac{t_{\delta/2, n-1} s}{\sqrt{n}} \leq p \leq \hat{p} + \frac{t_{\delta/2, n-1} s}{\sqrt{n}}$$
% where $t_{\delta/2, n-1}$ is the upper 100$\alpha/2$ percentage point
% of the $t$-distribution with $n-1$ degrees of freedom.
% \end{theorem}
% 
% Now, we demonstrate how we can use this theorem to establish an upper 
% and lower bound for the utility of a view, with probability $1 - \delta$.
% 
% Let the distance vector corresponding to the target view be:
% $\bar{a} = [a_1, a_2, \ldots, a_k]$ while the distance vector corresponding to
% the comparison view is:
% $\bar{b} = [b_1, b_2, \ldots, b_k]$.
% Notice that on very large datasets, it may be beneficial to precompute the
% distance vectors corresponding to the comparison views, so we assume that the
% vector $\bar{b}$ is computed exactly and known in advance.
% We let $a = \sum_i a_i$, and $ b = \sum_i b_i$.
% 
% Our goal is to use the sample to bound the values of the $a_i$ around $\ha_i$
% such that we can establish upper and lower bounds for the utilities.
% By applying Theorem~\ref{thm:confint}, we
% can get values $c_i$ for which $a_i \in [\ha_i - c_i, \ha_i + c_i]$
% with probability greater than $1 - \delta/k$.
% (By union bound, we will be able to ensure that all $a_i$'s
% are in their intervals with probability $1 - \delta$.)
% 
% Now, given these values $c_i$, we can establish an upper bound for the
% EMD (and also similarly for other distance metrics) in the following manner:
% We let $q_1(\bar{a}) = \sum_i \ha_i - c_i$, and $q_2(\bar{a}) = \sum_i \ha_i + c_i$.
% 
% 
% \begin{align*}
% EMD(\bar{a}, \bar{b}) & = \sum_i |a_i / a - b_i / b|\\
%           & = \sum_i |a_i / a - b_i / b|\\
%           & = 1/ab \sum_i \max (a_ib  - b_ia, b_ia - a_ib)\\
% \end{align*}
% Thus, we have:
% \begin{align}
% \frac{1}{b q_1(\bar{a})} \sum_i \max (a_ib  - b_ia, b_ia - a_ib) \leq & EMD(\bar{a}, \bar{b}) \leq \frac{1}{b q_2(\bar{a})} \sum_i \max (a_ib  - b_ia, b_ia - a_ib)\label{eq:emd}
% \end{align}
% 
% Note that: 
% \begin{align*}
% (\ha_i - c_i)b  - b_i (\sum_i (\ha_i + c_i)) & \leq a_ib  - b_ia  \leq (\ha_i + c_i)b  - b_i (\sum_i (\ha_i - c_i)), \textrm{\ and} \\
% b_i (\sum_i (\ha_i - c_i)) - (\ha_i + c_i) b & \leq b_i a  - a_i b  \leq  b_i (\sum_i (\ha_i + c_i)) - (\ha_i - c_i) b
% \end{align*}
% By plugging these quantities back into Eq~\ref{eq:emd},
% we have upper and lower bounds on the EMD metric.
% Similar mechanisms may be used to derive upper and lower bounds for other metrics.
% 
% Now that we have upper and lower bounds for the utility of each target view
% by evaluating the query on a sample,
% we can easily use it to prune away a number of views that are definitely not likely to be part 
% of the top-K,
% and instead focus on views that may be part of the top-K.

%   \subsection {Partitioning Tables}
%   The increase in the total execution time when a large number of queries are
%   executed in parallel suggests that there is a ``sweet spot'' with respect to
%   the maximum number of queries that can be run in parallel on a given table.
%   Therefore, we uniformly partition large tables into smaller ones and run
%   subsets of queries against each of the partitions. Note that the views
%   returned are nor approximate because we are now executing views against
%   subsets of the data. As a result, bounds developed in sampling now apply. We

 


%If a dimension attribute $\mathcal{d}$ is highly correlated with measure
  %attribute $\mathcal{m}$, then?

% \mpv{also from full paper draft}
% It is possible to collect the above statistics at the dataset level too, as
% opposed to the entire table level. The advantage of table level statistics is
% that they have to be computed only once per table; however, dataset-level
% statistics are more accurate since they only consider the specific parts of the
% table. XXX: we use dataset-level statistics with table statistics do not result
% in aggressive pruning. 



