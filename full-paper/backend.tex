%!TEX root=document.tex

\section{View Pruning}
\label{sec:pruning}

The previous sections described the \VizRecDB\ Execution Engine, the core part
of our system.
In this section, we discuss an important component of \VizRecDB\ that is invoked
even before the Execution Engine runs, namely the View Generator.
Given a user query $Q$, the purpose of the view generator module is to take the
input query, obtain metadata about the underlying tables and use correlations
between different columns in the table to prune views whose evaluation is
unnecessary. 
We distinguish between the pruning done in the execution engine and the pruning
done in the view generator: the execution engine prunes away low-utility views
while the view generator prunes away {\it redundant} views.
So what are redundant views? Redundant views are views that have similar
distributions and are therefore expected to have similar utility.
An extreme example is that of sales of a product as measured in US
Dollars and measured in Euros. Views with these measure attributes will be
identical and have the same utility.
Similarly, two dimension attributes, one corresponding to the airport name
and another corresponding to the airport code are guaranteed to produce identical
views irrespective of the measure attribute.
In both of the above cases, it suffices to compute and show only a single view
that is representative of multiple views (the frontend does list redundant
views that have been omitted).

The View Generator works in two stages: it performs view pruning offline and
identifies the set of viable views; then, when a new user query comes in, it
reads the set of viable views, performs pruning based on the input query and
passes view stubs on to the Execution Engine. 
The offline pruning does not depend on the input query and can therefore be
perfomed only once.
The offline stage works as follows. 
First, for each table, the View Generator obtains various types of
metadata including the data types of attributes, their classification into
measure and dimension attributes, number of distinct values for dimension
attributes, and the distributions for each measure attribute (mean, std
deviation).
The first piece of metadata is essential to determine the full space of
views. 
The number of distinct values for dimension attributes and the variance for
measure attributes is used to perform basic pruning of views (e.g. views
containing zero variance attributes will have low utility).
Next, the View Generator computes pairwise correlations between subsets of the
entire set of possible views.
To do so, the View Generator computes the aggregate distributions for all views
over the entire data (i.e. the comparison view). 
Once the distributions for all views have been computed, it calculates the
correlation between distributions of views that contain dimension attributes
with the same size (i.e. number of distinct values).
Specifically, since the distributions are numeric, it computes the pairwise
Pearson correlation between the distributions.
Next, it (conceptually) builds a graph of all views. 
The nodes of this graph correspond to views and an edge exists between two nodes
if the correlation between the two views is greater than a threshold (set to 0.95 in
our experiments). 
In this graph, the View Generator then identifies cliques (and almost cliques).
Observe that a clique in this graph is a set of highly correlated views,
and therefore, these views are likely to have similar utility. 
As a result, we select a
single node from each clique as a representative view for that clique and
prune the remaining views. 
We can think of this procedure as clustering views based on similarity and
choosing a representative view form each cluster.
At the end of the offline step, the View Generator stores the list of
viable views that must be evaluated at run time.

When the View Generator is invoked at runtime, it reads the list of viable
views for the table, prunes them further based on the input query (e.g.
attributes present in the where clause of the query should not be present in
any view) and passes the remaining views to the Execution Engine for
evaluation.

In real datasets, we find that the offline processing significantly
reduces the number of views that must be evaluated. 
For example, in the
diabetes dataset discussed in Section \ref{sec:experiments}, offline pruning
reduces the total number of views from XXX to XXX due to our pruning
strategy. Similarly, for the banking dataset, the total views are
reduced from XXX to XXX. In Figure \ref{}, we show graphs generated by the
View Generator for both datasets.

% \mpv{If two dimension attributes $a_i$ and $a_j$ have
% a high degree of correlation (e.g. full name of airport and abbreviated name of
% airport), the views generated by grouping the table on $a_i$ and $a_j$ will be
% very similar (and have almost equal utility). We can therefore generate and
% evaluate a single view representing both $a_i$ and $a_j$. \VizRecDB\ clusters
% attributes based on correlation and evaluates a representative view per
% cluster.}



  
% We next describe a scheme that allows us to associate upper and lower bounds for
% views by evaluating them on a small sample of the dataset.
% We describe the use of the scheme on a simple view where AVG(Y) for a given
% attribute Y is being computed for each group in attribute X.
% We can then depict this view using a bar chart or a histogram.
% 
% For this derivation, we assume that the AVG(Y) for any X = $x_i$, is normally
% distributed around a certain mean $p$.
% Given a number of samples for Y for X = $x_i$, we can employ the following
% theorem \cite{stats_book} to bound $p$ within a confidence interval with
% probability $1 - \delta$:
% \begin{theorem}~\label{thm:confint}
% If $\hat{p}$ and $s$ are the mean and standard deviation 
% of a random sample of size $n$ from a normal distribution with unknown 
% variance, a $1 - \delta$ probability confidence interval
% on $p$ is given by:
% $$\hat{p} - \frac{t_{\delta/2, n-1} s}{\sqrt{n}} \leq p \leq \hat{p} + \frac{t_{\delta/2, n-1} s}{\sqrt{n}}$$
% where $t_{\delta/2, n-1}$ is the upper 100$\alpha/2$ percentage point
% of the $t$-distribution with $n-1$ degrees of freedom.
% \end{theorem}
% 
% Now, we demonstrate how we can use this theorem to establish an upper 
% and lower bound for the utility of a view, with probability $1 - \delta$.
% 
% Let the distance vector corresponding to the target view be:
% $\bar{a} = [a_1, a_2, \ldots, a_k]$ while the distance vector corresponding to
% the comparison view is:
% $\bar{b} = [b_1, b_2, \ldots, b_k]$.
% Notice that on very large datasets, it may be beneficial to precompute the
% distance vectors corresponding to the comparison views, so we assume that the
% vector $\bar{b}$ is computed exactly and known in advance.
% We let $a = \sum_i a_i$, and $ b = \sum_i b_i$.
% 
% Our goal is to use the sample to bound the values of the $a_i$ around $\ha_i$
% such that we can establish upper and lower bounds for the utilities.
% By applying Theorem~\ref{thm:confint}, we
% can get values $c_i$ for which $a_i \in [\ha_i - c_i, \ha_i + c_i]$
% with probability greater than $1 - \delta/k$.
% (By union bound, we will be able to ensure that all $a_i$'s
% are in their intervals with probability $1 - \delta$.)
% 
% Now, given these values $c_i$, we can establish an upper bound for the
% EMD (and also similarly for other distance metrics) in the following manner:
% We let $q_1(\bar{a}) = \sum_i \ha_i - c_i$, and $q_2(\bar{a}) = \sum_i \ha_i + c_i$.
% 
% 
% \begin{align*}
% EMD(\bar{a}, \bar{b}) & = \sum_i |a_i / a - b_i / b|\\
%           & = \sum_i |a_i / a - b_i / b|\\
%           & = 1/ab \sum_i \max (a_ib  - b_ia, b_ia - a_ib)\\
% \end{align*}
% Thus, we have:
% \begin{align}
% \frac{1}{b q_1(\bar{a})} \sum_i \max (a_ib  - b_ia, b_ia - a_ib) \leq & EMD(\bar{a}, \bar{b}) \leq \frac{1}{b q_2(\bar{a})} \sum_i \max (a_ib  - b_ia, b_ia - a_ib)\label{eq:emd}
% \end{align}
% 
% Note that: 
% \begin{align*}
% (\ha_i - c_i)b  - b_i (\sum_i (\ha_i + c_i)) & \leq a_ib  - b_ia  \leq (\ha_i + c_i)b  - b_i (\sum_i (\ha_i - c_i)), \textrm{\ and} \\
% b_i (\sum_i (\ha_i - c_i)) - (\ha_i + c_i) b & \leq b_i a  - a_i b  \leq  b_i (\sum_i (\ha_i + c_i)) - (\ha_i - c_i) b
% \end{align*}
% By plugging these quantities back into Eq~\ref{eq:emd},
% we have upper and lower bounds on the EMD metric.
% Similar mechanisms may be used to derive upper and lower bounds for other metrics.
% 
% Now that we have upper and lower bounds for the utility of each target view
% by evaluating the query on a sample,
% we can easily use it to prune away a number of views that are definitely not likely to be part 
% of the top-K,
% and instead focus on views that may be part of the top-K.

%   \subsection {Partitioning Tables}
%   The increase in the total execution time when a large number of queries are
%   executed in parallel suggests that there is a ``sweet spot'' with respect to
%   the maximum number of queries that can be run in parallel on a given table.
%   Therefore, we uniformly partition large tables into smaller ones and run
%   subsets of queries against each of the partitions. Note that the views
%   returned are nor approximate because we are now executing views against
%   subsets of the data. As a result, bounds developed in sampling now apply. We

 


%If a dimension attribute $\mathcal{d}$ is highly correlated with measure
  %attribute $\mathcal{m}$, then?

% \mpv{also from full paper draft}
% It is possible to collect the above statistics at the dataset level too, as
% opposed to the entire table level. The advantage of table level statistics is
% that they have to be computed only once per table; however, dataset-level
% statistics are more accurate since they only consider the specific parts of the
% table. XXX: we use dataset-level statistics with table statistics do not result
% in aggressive pruning. 



